{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# about data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7613 entries, 0 to 7612\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        7613 non-null   int64 \n",
      " 1   keyword   7552 non-null   object\n",
      " 2   location  5080 non-null   object\n",
      " 3   text      7613 non-null   object\n",
      " 4   target    7613 non-null   int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 297.5+ KB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id             0\n",
       "keyword       61\n",
       "location    2533\n",
       "text           0\n",
       "target         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Nan values\n",
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['target'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4342</td>\n",
       "      <td>4323</td>\n",
       "      <td>2884</td>\n",
       "      <td>4342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3271</td>\n",
       "      <td>3229</td>\n",
       "      <td>2196</td>\n",
       "      <td>3271</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  keyword  location  text\n",
       "target                               \n",
       "0       4342     4323      2884  4342\n",
       "1       3271     3229      2196  3271"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.groupby('target').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, 'ablaze', 'accident', 'aftershock', 'airplane%20accident',\n",
       "       'ambulance', 'annihilated', 'annihilation', 'apocalypse',\n",
       "       'armageddon', 'army', 'arson', 'arsonist', 'attack', 'attacked',\n",
       "       'avalanche', 'battle', 'bioterror', 'bioterrorism', 'blaze',\n",
       "       'blazing', 'bleeding', 'blew%20up', 'blight', 'blizzard', 'blood',\n",
       "       'bloody', 'blown%20up', 'body%20bag', 'body%20bagging',\n",
       "       'body%20bags', 'bomb', 'bombed', 'bombing', 'bridge%20collapse',\n",
       "       'buildings%20burning', 'buildings%20on%20fire', 'burned',\n",
       "       'burning', 'burning%20buildings', 'bush%20fires', 'casualties',\n",
       "       'casualty', 'catastrophe', 'catastrophic', 'chemical%20emergency',\n",
       "       'cliff%20fall', 'collapse', 'collapsed', 'collide', 'collided',\n",
       "       'collision', 'crash', 'crashed', 'crush', 'crushed', 'curfew',\n",
       "       'cyclone', 'damage', 'danger', 'dead', 'death', 'deaths', 'debris',\n",
       "       'deluge', 'deluged', 'demolish', 'demolished', 'demolition',\n",
       "       'derail', 'derailed', 'derailment', 'desolate', 'desolation',\n",
       "       'destroy', 'destroyed', 'destruction', 'detonate', 'detonation',\n",
       "       'devastated', 'devastation', 'disaster', 'displaced', 'drought',\n",
       "       'drown', 'drowned', 'drowning', 'dust%20storm', 'earthquake',\n",
       "       'electrocute', 'electrocuted', 'emergency', 'emergency%20plan',\n",
       "       'emergency%20services', 'engulfed', 'epicentre', 'evacuate',\n",
       "       'evacuated', 'evacuation', 'explode', 'exploded', 'explosion',\n",
       "       'eyewitness', 'famine', 'fatal', 'fatalities', 'fatality', 'fear',\n",
       "       'fire', 'fire%20truck', 'first%20responders', 'flames',\n",
       "       'flattened', 'flood', 'flooding', 'floods', 'forest%20fire',\n",
       "       'forest%20fires', 'hail', 'hailstorm', 'harm', 'hazard',\n",
       "       'hazardous', 'heat%20wave', 'hellfire', 'hijack', 'hijacker',\n",
       "       'hijacking', 'hostage', 'hostages', 'hurricane', 'injured',\n",
       "       'injuries', 'injury', 'inundated', 'inundation', 'landslide',\n",
       "       'lava', 'lightning', 'loud%20bang', 'mass%20murder',\n",
       "       'mass%20murderer', 'massacre', 'mayhem', 'meltdown', 'military',\n",
       "       'mudslide', 'natural%20disaster', 'nuclear%20disaster',\n",
       "       'nuclear%20reactor', 'obliterate', 'obliterated', 'obliteration',\n",
       "       'oil%20spill', 'outbreak', 'pandemonium', 'panic', 'panicking',\n",
       "       'police', 'quarantine', 'quarantined', 'radiation%20emergency',\n",
       "       'rainstorm', 'razed', 'refugees', 'rescue', 'rescued', 'rescuers',\n",
       "       'riot', 'rioting', 'rubble', 'ruin', 'sandstorm', 'screamed',\n",
       "       'screaming', 'screams', 'seismic', 'sinkhole', 'sinking', 'siren',\n",
       "       'sirens', 'smoke', 'snowstorm', 'storm', 'stretcher',\n",
       "       'structural%20failure', 'suicide%20bomb', 'suicide%20bomber',\n",
       "       'suicide%20bombing', 'sunk', 'survive', 'survived', 'survivors',\n",
       "       'terrorism', 'terrorist', 'threat', 'thunder', 'thunderstorm',\n",
       "       'tornado', 'tragedy', 'trapped', 'trauma', 'traumatised',\n",
       "       'trouble', 'tsunami', 'twister', 'typhoon', 'upheaval',\n",
       "       'violent%20storm', 'volcano', 'war%20zone', 'weapon', 'weapons',\n",
       "       'whirlwind', 'wild%20fires', 'wildfire', 'windstorm', 'wounded',\n",
       "       'wounds', 'wreck', 'wreckage', 'wrecked'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['keyword'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, 'Birmingham', 'Est. September 2012 - Bristol', ...,\n",
       "       'Vancouver, Canada', 'London ', 'Lincoln'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['location'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA and feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['has_location'] = train_df['location'].notnull().astype('int')\n",
    "train_df['has_keyword'] = train_df['keyword'].notnull().astype('int')\n",
    "train_df['text_len'] = train_df['text'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "has_location\n",
       "0    0.424398\n",
       "1    0.432283\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.groupby('has_location')['target'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "has_keyword\n",
       "0    0.688525\n",
       "1    0.427569\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.groupby('has_keyword')['target'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0     95.706817\n",
       "1    108.113421\n",
       "Name: text_len, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.groupby('target')['text_len'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    101.0\n",
       "1    115.0\n",
       "Name: text_len, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.groupby('target')['text_len'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2a70d90d0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df.groupby('target')['text_len'].plot(kind='hist', alpha=0.5, bins=50)\n",
    "plt.legend(['0', '1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data\n",
    "test_df['has_location'] = test_df['location'].notnull().astype('int')\n",
    "test_df['has_keyword'] = test_df['keyword'].notnull().astype('int')\n",
    "test_df['text_len'] = test_df['text'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "1. simple feature model (lightGBM)\n",
    "2. tf-idf model (lightGBM)\n",
    "3. sentence-BERT embedding (lightGBM)\n",
    "4. fine-tuning (BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5909389363099147\n",
      "0.45684394071490847\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.73      0.67       874\n",
      "           1       0.53      0.40      0.46       649\n",
      "\n",
      "    accuracy                           0.59      1523\n",
      "   macro avg       0.57      0.57      0.56      1523\n",
      "weighted avg       0.58      0.59      0.58      1523\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model with simple features (lightgbm)\n",
    "# split train_df into train and validation\n",
    "X_simple = train_df[['has_location', 'has_keyword', 'text_len']]\n",
    "y_simple = train_df['target']\n",
    "X_train_simple, X_val_simple, y_train_simple, y_val_simple = train_test_split(X_simple, y_simple, test_size=0.2, random_state=42)\n",
    "\n",
    "# train model\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "lgb_train_simple = lgb.Dataset(X_train_simple, y_train_simple)\n",
    "lgb_val_simple = lgb.Dataset(X_val_simple, y_val_simple)\n",
    "\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'verbosity': -1,\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5\n",
    "}\n",
    "\n",
    "model_simple = lgb.train(\n",
    "    params,\n",
    "    lgb_train_simple,\n",
    "    valid_sets=[lgb_train_simple, lgb_val_simple],\n",
    "    num_boost_round=1000\n",
    ")\n",
    "\n",
    "# predict\n",
    "y_pred_simple = model_simple.predict(X_val_simple)\n",
    "y_pred_simple = np.round(y_pred_simple)\n",
    "\n",
    "# evaluate\n",
    "print(accuracy_score(y_val_simple, y_pred_simple))\n",
    "print(f1_score(y_val_simple, y_pred_simple))\n",
    "print(classification_report(y_val_simple, y_pred_simple))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7590282337491793\n",
      "0.7094220110847188\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.81      0.79       874\n",
      "           1       0.73      0.69      0.71       649\n",
      "\n",
      "    accuracy                           0.76      1523\n",
      "   macro avg       0.75      0.75      0.75      1523\n",
      "weighted avg       0.76      0.76      0.76      1523\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model with tf-idf (lightgbm)\n",
    "# split train_df into train and validation\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_text = vectorizer.fit_transform(train_df['text'])\n",
    "y_text = train_df['target']\n",
    "X_train_text, X_val_text, y_train_text, y_val_text = train_test_split(X_text, y_text, test_size=0.2, random_state=42)\n",
    "\n",
    "# train model\n",
    "lgb_train_text = lgb.Dataset(X_train_text, y_train_text)\n",
    "lgb_val_text = lgb.Dataset(X_val_text, y_val_text)\n",
    "\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'verbosity': -1,\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5\n",
    "}\n",
    "\n",
    "model_text = lgb.train(\n",
    "    params,\n",
    "    lgb_train_text,\n",
    "    valid_sets=[lgb_train_text, lgb_val_text],\n",
    "    num_boost_round=1000\n",
    ")\n",
    "\n",
    "# predict\n",
    "y_pred_text = model_text.predict(X_val_text)\n",
    "y_pred_text = np.round(y_pred_text)\n",
    "\n",
    "# evaluate\n",
    "print(accuracy_score(y_val_text, y_pred_text))\n",
    "print(f1_score(y_val_text, y_pred_text))\n",
    "print(classification_report(y_val_text, y_pred_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8181221273801708\n",
      "0.7757085020242914\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.88      0.85       874\n",
      "           1       0.82      0.74      0.78       649\n",
      "\n",
      "    accuracy                           0.82      1523\n",
      "   macro avg       0.82      0.81      0.81      1523\n",
      "weighted avg       0.82      0.82      0.82      1523\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sentence-BERT model from huggingface\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "\n",
    "# encode text\n",
    "X_text = train_df['text'].tolist()\n",
    "X_text = model.encode(X_text)\n",
    "\n",
    "# split train_df into train and validation\n",
    "y_text = train_df['target']\n",
    "X_train_text, X_val_text, y_train_text, y_val_text = train_test_split(X_text, y_text, test_size=0.2, random_state=42)\n",
    "\n",
    "# train model\n",
    "lgb_train_text = lgb.Dataset(X_train_text, y_train_text)\n",
    "lgb_val_text = lgb.Dataset(X_val_text, y_val_text)\n",
    "\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'verbosity': -1,\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5\n",
    "}\n",
    "\n",
    "model_text = lgb.train(\n",
    "    params,\n",
    "    lgb_train_text,\n",
    "    valid_sets=[lgb_train_text, lgb_val_text],\n",
    "    num_boost_round=1000\n",
    ")\n",
    "\n",
    "# predict\n",
    "y_pred_text = model_text.predict(X_val_text)\n",
    "y_pred_text = np.round(y_pred_text)\n",
    "\n",
    "# evaluate\n",
    "print(accuracy_score(y_val_text, y_pred_text))\n",
    "print(f1_score(y_val_text, y_pred_text))\n",
    "print(classification_report(y_val_text, y_pred_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = train_df['text_len'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine-tune BERT model from huggingface\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# split train_df into train and validation\n",
    "X_text = train_df['text'].values\n",
    "y_text = train_df['target'].values\n",
    "\n",
    "X_train_text, X_val_text, y_train_text, y_val_text = train_test_split(X_text, y_text, test_size=0.2, random_state=42)\n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kiro02/Library/Caches/pypoetry/virtualenvs/cub-2023oct-tIHWYxGD-py3.11/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def convert_to_input(texts, labels):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    # For every tweet...\n",
    "    for tweet in texts:\n",
    "        # `encode_plus` will:\n",
    "        #   (1) Tokenize the sentence.\n",
    "        #   (2) Prepend the `[CLS]` token to the start.\n",
    "        #   (3) Append the `[SEP]` token to the end.\n",
    "        #   (4) Map tokens to their IDs.\n",
    "        #   (5) Pad or truncate the sentence to `max_length`\n",
    "        #   (6) Create attention masks for [PAD] tokens.\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "                            tweet,                      # Sentence to encode.\n",
    "                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                            max_length = max_len,           # Pad & truncate all sentences.\n",
    "                            pad_to_max_length = True,\n",
    "                            return_attention_mask = True,   # Construct attn. masks.\n",
    "                            return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                    )\n",
    "        \n",
    "        # Add the encoded sentence to the list.    \n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        \n",
    "        # And its attention mask (simply differentiates padding from non-padding).\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "    # Convert the lists into tensors.\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    labels = torch.tensor(labels)\n",
    "\n",
    "    # Combine the training inputs into a TensorDataset.\n",
    "    dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# convert to input\n",
    "train_dataset = convert_to_input(X_train_text, y_train_text)\n",
    "val_dataset = convert_to_input(X_val_text, y_val_text)\n",
    "test_dataset = convert_to_input(test_df['text'].values, np.zeros(test_df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "# We'll take training samples in random order. \n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )\n",
    "\n",
    "# for test data\n",
    "test_dataloader = DataLoader(\n",
    "            test_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(test_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors: 100%|██████████| 440M/440M [00:20<00:00, 22.0MB/s] \n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
    "# linear classification layer on top. \n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "                    # You can increase this for multi-class tasks.   \n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")\n",
    "\n",
    "device = torch.device(\"mps\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kiro02/Library/Caches/pypoetry/virtualenvs/cub-2023oct-tIHWYxGD-py3.11/lib/python3.11/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )\n",
    "\n",
    "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
    "# We chose to run for 4, but we'll see later that this may be over-fitting the\n",
    "# training data.\n",
    "epochs = 4\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.63\n",
      "  Training epcoh took: 0:03:27\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.66\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.58\n",
      "  Training epcoh took: 0:03:25\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.73\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.51\n",
      "  Training epcoh took: 0:03:27\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.73\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.48\n",
      "  Training epcoh took: 0:03:22\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.73\n",
      "\n",
      "Training complete!\n",
      "Total training took 0:14:45 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "training_stats = []\n",
    "\n",
    "# Measure the total training time for the whole run.\n",
    "total_t0 = time.time()\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    # Perform one full pass over the training set.\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "    total_train_loss = 0\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the device using the \n",
    "        # `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(b_input_ids, \n",
    "                             token_type_ids=None, \n",
    "                             attention_mask=b_input_mask, \n",
    "                             labels=b_labels)        \n",
    "        loss = output.loss\n",
    "        total_train_loss += loss.item()\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "        # modified based on their gradients, the learning rate, etc.\n",
    "        optimizer.step()\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    \n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "    t0 = time.time()\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    model.eval()\n",
    "    # Tracking variables \n",
    "    total_eval_accuracy = 0\n",
    "    best_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        # Tell pytorch not to bother with constructing the compute graph during\n",
    "        # the forward pass, since this is only needed for backprop (training).\n",
    "        with torch.no_grad():        \n",
    "            output= model(b_input_ids, \n",
    "                                   token_type_ids=None, \n",
    "                                   attention_mask=b_input_mask,\n",
    "                                   labels=b_labels)\n",
    "        loss = output.loss\n",
    "        total_eval_loss += loss.item()\n",
    "        # Move logits and labels to CPU if we are using GPU\n",
    "        logits = output.logits\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        # Calculate the accuracy for this batch of test sentences, and\n",
    "        # accumulate it over all batches.\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "    # Report the final accuracy for this validation run.\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    # Measure how long the validation run took.\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    if avg_val_accuracy > best_eval_accuracy:\n",
    "        torch.save(model, 'bert_model')\n",
    "        best_eval_accuracy = avg_val_accuracy\n",
    "    #print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    #print(\"  Validation took: {:}\".format(validation_time))\n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on test data by lightgbm\n",
    "vectorizer = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "\n",
    "test_vector = vectorizer.encode(test_df['text'])\n",
    "y_pred_text = model_text.predict(test_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3263,)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['label'] = y_pred_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[['id', 'label']].rename(\n",
    "    columns={'id': 'id', 'label': 'target'}\n",
    ").to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cub-2023oct-tIHWYxGD-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
